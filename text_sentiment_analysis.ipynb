{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM4zUzH+FeGQPQIRKG4oxd6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pp7CaIPGcj5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import plotly.express as px\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUNkPmpCI0jC",
        "outputId": "a7d61db6-bc6d-454b-afec-e0a3710cc92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: process data\n",
        "def text_processing(df):\n",
        "\n",
        "  # Remove punctuation\n",
        "  def remove_punc(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "  \n",
        "  # Update data\n",
        "  df['Text Without Punctuation'] = df['Text'].apply(remove_punc)\n",
        "\n",
        "  # Filter out stopwords\n",
        "  def preprocess(text):\n",
        "    \n",
        "    final = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "      if len(token) > 2 and token not in stopwords:\n",
        "        final.append(token)\n",
        "    return final\n",
        "\n",
        "  # Update data\n",
        "  df['Text Without Punctuation and Stopwords'] = df['Text Without Punctuation'].apply(preprocess)\n",
        "\n",
        "  # Get list and number of words in data\n",
        "  words_list = []\n",
        "  for words in df['Text Without Punctuation and Stopwords']:\n",
        "    for word in words:\n",
        "      words_list.append(word)\n",
        "  num_words = len(set(words_list))\n",
        "  \n",
        "  return df"
      ],
      "metadata": {
        "id": "ystpZjFwGrhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_analysis(df):\n",
        "\n",
        "  df = text_processing(df)\n",
        "\n",
        "  X = df['Text Without Punctuation and Stopwords']\n",
        "  y = df['Sentiment']\n",
        "\n",
        "  # Step 2: tokenizing and padding\n",
        "\n",
        "  # Choose proportion of data to train and test\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
        "\n",
        "  # Tokenize words\n",
        "  tokenizer = Tokenizer(num_words)\n",
        "  tokenizer.fit_on_texts(X_train)\n",
        "  \n",
        "  train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "  test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "  # Padding\n",
        "  padded_train = pad_sequences(train_sequences, maxlen = 29, padding = 'post', truncating = 'post')\n",
        "  padded_test = pad_sequences(test_sequences, maxlen = 29, truncating = 'post')\n",
        "\n",
        "  # Categorical 2D representation \n",
        "  cat_y_train = to_categorical(y_train, 2)\n",
        "  cat_y_test = to_categorical(y_test, 2)\n",
        "\n",
        "  # Sequential Model\n",
        "  model = Sequential()\n",
        "\n",
        "  # Embedding layer\n",
        "  model.add(Embedding(num_words, output_dim = 512))\n",
        "\n",
        "  model.add(LSTM(256))\n",
        "\n",
        "  # Dense layers\n",
        "  model.add(Dense(128, activation = 'relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(2, activation = 'softmax'))\n",
        "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
        "\n",
        "  # Step 3: train the model\n",
        "  model.fit(padded_train, cat_y_train, batch_size = 32, validation_split = 0.2, epochs = 2)\n",
        "\n",
        "  # Step 4: test the model\n",
        "  predicted = model.predict(padded_test)\n",
        "\n",
        "  prediction = []\n",
        "  for i in predicted:\n",
        "    prediction.append(np.argmax(i))\n",
        "\n",
        "  original = []\n",
        "  for i in cat_y_test:\n",
        "    original.append(np.argmax(i))\n",
        "\n",
        "  # Step 5: analysis of the model\n",
        "  accuracy = accuracy_score(original, prediction)\n",
        "  \n",
        "  sns.heatmap(confusion_matrix(original, prediction), annot = True)\n",
        "\n",
        "  return print(\"The model has an accuracy of:\", accuracy), sns.heatmap(confusion_matrix(original, prediction), annot = True)\n"
      ],
      "metadata": {
        "id": "MBXs-fMiKngH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}